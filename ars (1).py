# -*- coding: utf-8 -*-
"""ARS

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZSPxFOUUBDHgPAw0CruyHkBN-yHwYbA3
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load datasets
train_data = pd.read_csv('Dev_data_to_be_shared.csv')
validation_data = pd.read_csv('validation_data_to_be_shared.csv')

# # Explore schema
# print("Train Data Shape:", train_data.shape)
# print("Validation Data Shape:", validation_data.shape)
# print("Train Data Columns:", train_data.columns.tolist())

# Check data types
print(train_data.dtypes.value_counts())
print(train_data.info())

# Summarize numerical columns
print(train_data.describe().T)

# Identify column groups by prefix
transaction_cols = [col for col in train_data.columns if col.startswith('transaction_attribute')]
bureau_cols = [col for col in train_data.columns if col.startswith('bureau')]
bureau_enquiry_cols = [col for col in train_data.columns if col.startswith('bureau_enquiry')]
onus_cols = [col for col in train_data.columns if col.startswith('onus_attribute')]

print(f"Transaction Attributes: {len(transaction_cols)}")
print(f"Bureau Attributes: {len(bureau_cols)}")
print(f"Bureau Enquiry Attributes: {len(bureau_enquiry_cols)}")
print(f"Onus Attributes: {len(onus_cols)}")

# Analyze target variable
bad_flag_counts = train_data['bad_flag'].value_counts(normalize=True)
print("Bad Flag Distribution:")
print(bad_flag_counts)

# Plot target variable distribution
sns.barplot(x=bad_flag_counts.index, y=bad_flag_counts.values)
plt.title("Distribution of bad_flag (Target Variable)")
plt.xlabel("Bad Flag")
plt.ylabel("Proportion")
plt.show()

# Number of unique values in bad flag column and count of each

unique_bad_flags = train_data['bad_flag'].unique()
print(f"Number of unique values in 'bad_flag' column: {len(unique_bad_flags)}")
print(f"Count of each unique value:\n{train_data['bad_flag'].value_counts()}")

# Calculate the percentage of null values in each column
null_percentages = train_data.isnull().sum() * 100 / len(train_data)

# Print the results
print("Percentage of null values in each column:")
null_percentages

# Identify columns with more than 10% null values
columns_with_high_nulls = null_percentages[null_percentages > 25]
print("\nColumns with more than 10% null values:")
columns_with_high_nulls

# Drop columns with more than 10% null values
train_data = train_data.drop(columns=columns_with_high_nulls.index)
validation_data = validation_data.drop(columns=columns_with_high_nulls.index)

train_data

validation_data

# Impute missing values using the mean for numerical columns
numerical_cols = train_data.select_dtypes(include=['number']).columns
for col in numerical_cols:
    if train_data[col].isnull().any():
        train_data[col] = train_data[col].fillna(train_data[col].mean())
        validation_data[col] = validation_data[col].fillna(validation_data[col].mean())

# Impute missing values using the mode for categorical columns
categorical_cols = train_data.select_dtypes(include=['object']).columns
for col in categorical_cols:
    if train_data[col].isnull().any():
        train_data[col] = train_data[col].fillna(train_data[col].mode()[0])
        validation_data[col] = validation_data[col].fillna(validation_data[col].mode()[0])

# Calculate the percentage of null values in each column
null_percentages = train_data.isnull().sum() * 100 / len(train_data)

# Print the results
print("Percentage of null values in each column:")
null_percentages

train_data

# Normalize variables with extreme distributions if necessary.

from sklearn.preprocessing import MinMaxScaler

# Identify numerical features to normalize (excluding 'bad_flag' and 'account_number')
numerical_features = train_data.select_dtypes(include=['number']).columns
numerical_features = numerical_features.drop(['bad_flag', 'account_number']) if 'bad_flag' in numerical_features and 'account_number' in numerical_features else numerical_features

# Initialize MinMaxScaler
scaler = MinMaxScaler()

# Fit and transform the numerical features in the training data
train_data[numerical_features] = scaler.fit_transform(train_data[numerical_features])

# Transform the numerical features in the validation data using the same scaler
validation_data[numerical_features] = scaler.transform(validation_data[numerical_features])

# Calculate the correlation matrix
correlation_matrix = train_data.corr()

# Set a threshold for high correlation (e.g., 0.8)
threshold = 0.85

# Find pairs of columns with correlation above the threshold
high_correlation_pairs = []
for i in range(len(correlation_matrix.columns)):
    for j in range(i + 1, len(correlation_matrix.columns)):
        if abs(correlation_matrix.iloc[i, j]) > threshold:
            col1 = correlation_matrix.columns[i]
            col2 = correlation_matrix.columns[j]
            high_correlation_pairs.append((col1, col2, correlation_matrix.iloc[i, j]))

# # Print the pairs of columns with high correlation
# print("Columns with high correlation:")
# for pair in high_correlation_pairs:
#     print(f"{pair[0]} and {pair[1]}: {pair[2]}")

# Identify columns to drop based on high correlation
columns_to_drop = []
for pair in high_correlation_pairs:
    col1, col2, correlation = pair
    if col1 not in columns_to_drop and col2 not in columns_to_drop:
        # Choose the column to drop based on a criterion (e.g., lower absolute correlation with the target variable)
        # Here, we simply drop the second column in the pair
        columns_to_drop.append(col2)

# Drop the selected columns from the training and validation datasets
train_data = train_data.drop(columns=columns_to_drop)
validation_data = validation_data.drop(columns=columns_to_drop)

train_data
from imblearn.over_sampling import SMOTE

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, GridSearchCV,RandomizedSearchCV
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from sklearn.metrics import roc_auc_score, classification_report

# 1. Data Splitting
X_validation = validation_data.drop(columns=[ 'account_number'])  # Features
X_validation

X = train_data.drop(columns=['bad_flag', 'account_number'])  # Features
y = train_data['bad_flag']

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import RandomizedSearchCV
import xgboost as xgb
from lightgbm import LGBMClassifier
from sklearn.neural_network import MLPClassifier

# Define common evaluation function
from sklearn.metrics import roc_auc_score, classification_report

def evaluate_model(model, X_train, y_train, X_test, y_test):
    # Train the model
    model.fit(X_train, y_train)

    # Predict probabilities
    y_pred_prob = model.predict_proba(X_test)[:, 1]
    y_pred = model.predict(X_test)

    # Metrics
    auc = roc_auc_score(y_test, y_pred_prob)
    print(f"ROC AUC Score: {auc:.4f}")
    print("Classification Report:")
    print(classification_report(y_test, y_pred))
    return auc

# Initialize models
rf = RandomForestClassifier(random_state=42, n_jobs=-1)
xgb_model = xgb.XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss')
lgbm = LGBMClassifier(random_state=42)
mlp = MLPClassifier(random_state=42, max_iter=500)

# Evaluate Random Forest
print("\n--- Random Forest ---")
rf_auc = evaluate_model(rf, X_train, y_train, X_test, y_test)

# Evaluate XGBoost
print("\n--- XGBoost ---")
xgb_auc = evaluate_model(xgb_model, X_train, y_train, X_test, y_test)

# Evaluate LightGBM
print("\n--- LightGBM ---")
lgbm_auc = evaluate_model(lgbm, X_train, y_train, X_test, y_test)

# Evaluate Neural Network
print("\n--- Neural Network (MLP) ---")
mlp_auc = evaluate_model(mlp, X_train, y_train, X_test, y_test)

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix, roc_curve
import matplotlib.pyplot as plt


# y = train_data['bad_flag']  # Target variable

smote = SMOTE(random_state=42)
X, y = smote.fit_resample(X, y)

# Split into training (70%) and testing (30%) subsets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)

# 2. Baseline Logistic Regression
# Initialize the logistic regression model
# log_reg = LogisticRegression(max_iter=1000, random_state=42)

# # Train the model
# log_reg.fit(X_train, y_train)

# # Make predictions
# y_pred = log_reg.predict(X_test)
# y_pred_prob = log_reg.predict_proba(X_test)[:, 1]

# # Evaluate the model
# print("Classification Report:")
# print(classification_report(y_test, y_pred))

# print(f"ROC AUC Score: {roc_auc_score(y_test, y_pred_prob)}")

# # Confusion matrix
# conf_matrix = confusion_matrix(y_test, y_pred)
# print("Confusion Matrix:")
# print(conf_matrix)

# # Plot the ROC Curve
# fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)
# plt.figure(figsize=(8, 6))
# plt.plot(fpr, tpr, color='blue', label=f"ROC Curve (AUC = {roc_auc_score(y_test, y_pred_prob):.2f})")
# plt.plot([0, 1], [0, 1], color='red', linestyle='--', label="Random Classifier")
# plt.xlabel("False Positive Rate")
# plt.ylabel("True Positive Rate")
# plt.title("ROC Curve")
# plt.legend()
# plt.show()

# Model Training and Tuning
models = {
    # 'Logistic Regression': LogisticRegression(max_iter=1000),
    'Random Forest': RandomForestClassifier()
    # 'Gradient Boosting': GradientBoostingClassifier()
}

# best_model = None
# best_score = 0

param_distributions = {
    'Random Forest': {
        'n_estimators': [100],
        'max_depth': [10],
     }
    # 'Gradient Boosting': {
    #     'n_estimators': [100],
    #     'learning_rate': [0.01],
    # }
}

for model_name, model in models.items():
    if model_name in param_distributions:
        random_search = RandomizedSearchCV(
            model,
            param_distributions=param_distributions[model_name],
            scoring='roc_auc',
            cv=5,
            n_iter=10,
            random_state=42
        )
        random_search.fit(X_train, y_train)
        model = random_search.best_estimator_

    model.fit(X_train, y_train)
    preds = model.predict_proba(X_test)[:, 1]
    score = roc_auc_score(y_test, preds)

    print(f"{model_name} ROC AUC: {score}")

    # Final Model Evaluation
    final_preds = model.predict_proba(X_test)[:, 1]
    print("Final Model Classification Report:")
    print(classification_report(y_test, final_preds > 0.5))

    # Validation Data Predictions
    validation_preds = model.predict_proba(X_validation)[:, 1]

    # Output Results
    results = pd.DataFrame({
        'account_number': validation_data['account_number'],
        'predicted_probability': validation_preds
    })

    results.to_csv("validation_predictions.csv", index=False)

# Plot the ROC Curve
fpr, tpr, thresholds = roc_curve(y_test, final_preds)
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='blue', label=f"ROC Curve (AUC = {roc_auc_score(y_test, final_preds):.2f})")
plt.plot([0, 1], [0, 1], color='red', linestyle='--', label="Random Classifier")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve")
plt.legend()
plt.show()

